{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import sys\n", "import time\n", "import logging\n", "import multiprocessing\n", "from typing import Dict\n", "from typing import Dict, Generator\n", "from typing import Dict, List, Generator\n", "\n", "import cv2\n", "import dask\n", "import skimage.io\n", "import numpy as np\n", "import pandas as pd\n", "import tensorflow as tf\n", "import dask.array as da\n", "import dask.dataframe as dd\n", "\n", "from perceptilabs.core_new.graph import Graph\n", "from perceptilabs.core_new.utils import Picklable\n", "from perceptilabs.core_new.layers.base import Tf1xLayer\n", "from perceptilabs.core_new.layers.base import DataRandom\n", "from perceptilabs.core_new.layers.base import DataSupervised\n", "from perceptilabs.core_new.utils import Picklable, YieldLevel\n", "from tensorflow.python.training.tracking.base import Trackable\n", "from perceptilabs.core_new.communication import TrainingServer\n", "from perceptilabs.core_new.layers.base import GANLayer, Tf1xLayer\n", "from perceptilabs.core_new.serialization import can_serialize, serialize\n", "from perceptilabs.core_new.graph.builder import GraphBuilder, SnapshotBuilder\n", "from perceptilabs.messaging import ZmqMessagingFactory, SimpleMessagingFactory\n", "from perceptilabs.core_new.layers.replication import BASE_TO_REPLICA_MAP, REPLICATED_PROPERTIES_TABLE\n", "\n", "logging.basicConfig(\n", "    stream=sys.stdout,\n", "    format='%(asctime)s - %(levelname)s - %(message)s',\n", "    level=logging.INFO\n", ")\n", "log = logging.getLogger(__name__)\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import numpy as np\n", "class DataRandom_Random_1(DataRandom):\n", "    \"\"\"Class responsible for generating random noise\"\"\"    \n", "    def __init__(self):\n", "        self._variables = {}\n", "        self._distribution = 'Normal'\n", "        self._columns = []\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "    @property\n", "    def size_training(self) -> int:\n", "        \"\"\"Returns the size of the training dataset\"\"\"                    \n", "        return 1\n", "\n", "    @property\n", "    def size_validation(self) -> int:\n", "        \"\"\"Returns the size of the validation dataset\"\"\"\n", "        return 1\n", "\n", "    @property\n", "    def size_testing(self) -> int:\n", "        \"\"\"Returns the size of the testing dataset\"\"\"                    \n", "        return 1\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Returns any variables that the layer should make available and that can be pickled.\"\"\"\n", "        return self._variables\n", "\n", "    @property\n", "    def sample(self) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"                    \n", "        sample = next(self.make_generator_training())\n", "        return sample\n", "\n", "    @property\n", "    def columns(self) -> List[str]:\n", "        \"\"\"Column names. Corresponds to each column in a sample \"\"\"\n", "        return self._columns.copy()\n", "              \n", "    def make_generator_training(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a sample of random noise data.\"\"\"                                        \n", "        def gen():\n", "            rdn_state = np.random.RandomState(1111)\n", "            while True:\n", "                sample = rdn_state.normal(0.1, 0.5, (100,))\n", "                yield {'output': np.float32(sample)}\n", "        return gen()\n", "    \n", "    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a sample of random noise data.\"\"\"                                        \n", "        def gen():\n", "            rdn_state = np.random.RandomState(1234)\n", "            while True:\n", "                sample = rdn_state.normal(0.1, 0.5, (100,))\n", "                yield {'output': np.float32(sample)}\n", "        return gen()    \n", "        \n", "    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a sample of random noise data.\"\"\"                                        \n", "        def gen():\n", "            rdn_state = np.random.RandomState(5678)\n", "            while True:\n", "                sample = rdn_state.normal(0.1, 0.5, (100,))\n", "                yield {'output': np.float32(sample)}\n", "        return gen()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepLearningFC_Dense_1(Tf1xLayer):\n", "    def __init__(self):\n", "        self._scope = 'DeepLearningFC_Dense_1'\n", "        self._n_neurons = 128\n", "        self._variables = {}\n", "        self._keep_prob = 1\n", "\n", "    def __call__(self, inputs: Dict[str, tf.Tensor], is_training: tf.Tensor = None) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor.\"\"\"\n", "        \n", "                            \n", "                \n", "        x = inputs['input']\n", "        n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)\n", "        b_norm = False\n", "        y_before = None\n", "        y = None\n", "\n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            is_training = tf.constant(True) if is_training is None else is_training\n", "            \n", "            initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)\n", "            W = tf.compat.v1.get_variable('W', initializer=initial)\n", "\n", "            initial = tf.constant(0., shape=[self._n_neurons])\n", "            b = tf.compat.v1.get_variable('b', initializer=initial)\n", "            flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)\n", "            y = tf.matmul(flat_node, W) + b\n", "\n", "            y_before = y\n", "            \n", "\n", "            y = tf.compat.v1.sigmoid(y)\n", "        \n", "\n", "            \n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "            \n", "        self._outputs = {            \n", "            'output': y,\n", "            'y_before': y_before,\n", "            'initial': tf.expand_dims(initial, axis=0),            \n", "            'W': tf.expand_dims(W, axis=0),            \n", "            'b': tf.expand_dims(b, axis=0),            \n", "            'flat_node': tf.expand_dims(flat_node, axis=0),            \n", "        }\n", "                            \n", "\n", "        return self._outputs\n", "\n", "    def get_sample(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Returns a dictionary of sample tensors\n", "\n", "        Returns:\n", "            A dictionary of sample tensors\n", "        \"\"\"\n", "        return self._outputs\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()\n", "\n", "    @property\n", "    def trainable_variables(self):\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)\n", "        variables = {v.name: v for v in variables}\n", "        return variables\n", "\n", "    @property\n", "    def weights(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            w = tf.compat.v1.get_variable('W')\n", "            return {w.name: w}\n", "\n", "    @property\n", "    def biases(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            b = tf.compat.v1.get_variable('b')\n", "            return {b.name: b}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepLearningFC_Dense_2(Tf1xLayer):\n", "    def __init__(self):\n", "        self._scope = 'DeepLearningFC_Dense_2'\n", "        self._n_neurons = 784\n", "        self._variables = {}\n", "        self._keep_prob = 1\n", "\n", "    def __call__(self, inputs: Dict[str, tf.Tensor], is_training: tf.Tensor = None) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor.\"\"\"\n", "        \n", "                            \n", "                \n", "        x = inputs['input']\n", "        n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)\n", "        b_norm = False\n", "        y_before = None\n", "        y = None\n", "\n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            is_training = tf.constant(True) if is_training is None else is_training\n", "            \n", "            initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)\n", "            W = tf.compat.v1.get_variable('W', initializer=initial)\n", "\n", "            initial = tf.constant(0., shape=[self._n_neurons])\n", "            b = tf.compat.v1.get_variable('b', initializer=initial)\n", "            flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)\n", "            y = tf.matmul(flat_node, W) + b\n", "\n", "            y_before = y\n", "            \n", "\n", "            y = tf.compat.v1.sigmoid(y)\n", "        \n", "\n", "            \n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "            \n", "        self._outputs = {            \n", "            'output': y,\n", "            'y_before': y_before,\n", "            'initial': tf.expand_dims(initial, axis=0),            \n", "            'W': tf.expand_dims(W, axis=0),            \n", "            'b': tf.expand_dims(b, axis=0),            \n", "            'flat_node': tf.expand_dims(flat_node, axis=0),            \n", "        }\n", "                            \n", "\n", "        return self._outputs\n", "\n", "    def get_sample(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Returns a dictionary of sample tensors\n", "\n", "        Returns:\n", "            A dictionary of sample tensors\n", "        \"\"\"\n", "        return self._outputs\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()\n", "\n", "    @property\n", "    def trainable_variables(self):\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)\n", "        variables = {v.name: v for v in variables}\n", "        return variables\n", "\n", "    @property\n", "    def weights(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            w = tf.compat.v1.get_variable('W')\n", "            return {w.name: w}\n", "\n", "    @property\n", "    def biases(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            b = tf.compat.v1.get_variable('b')\n", "            return {b.name: b}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ProcessReshape_Reshape_1(Tf1xLayer):\n", "    def __call__(self, inputs: Dict[str, tf.Tensor], is_training: tf.Tensor = None) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Takes a tensor as input and reshapes it.\"\"\"\n", "        \n", "                            \n", "        \n", "        shape = list((28, 28, 1))\n", "        permutation = list((0, 1, 2))\n", "        \n", "        shape = [i for i in shape if i != 0]\n", "        if(len(shape) != len(permutation)):\n", "            permutation = []\n", "            for i in range(len(shape)):\n", "                permutation.append(i)\n", "\n", "        x = inputs['input']\n", "        input_shape = x.get_shape().as_list() \n", "        new_shape = [input_shape[0] if input_shape[0] is not None else -1] + shape\n", "        \n", "        y = tf.reshape(x, new_shape)\n", "        y = tf.transpose(y, perm=[0] + [i+1 for i in permutation])\n", "\n", "            \n", "        self._outputs = {'output': y}\n", "                \n", "\n", "        return self._outputs\n", "\n", "    def get_sample(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Returns a dictionary of sample tensors\n", "\n", "        Returns:\n", "            A dictionary of sample tensors\n", "        \"\"\"\n", "        return self._outputs\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return {}\n", "\n", "    @property\n", "    def trainable_variables(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        return {}\n", "    \n", "    @property\n", "    def weights(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n", "\n", "    @property\n", "    def biases(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataData_MNIST(DataSupervised):\n", "    \"\"\"Class responsible for loading data from files (e.g., numpy, csv, etc).\"\"\"    \n", "    def __init__(self):\n", "        self._variables = {}\n", "        self._columns = []\n", "        self._stored_sample = None\n", "\n", "        columns = {}\n", "        trn_sz_tot, val_sz_tot, tst_sz_tot = 0, 0, 0        \n", "        trn_gens_args_DataData_MNIST, val_gens_args_DataData_MNIST, tst_gens_args_DataData_MNIST = [], [], []   \n", "    \n", "  \n", "\n", "        columns_DataData_MNIST_0 = None\n", "\n", "        global matrix_DataData_MNIST_0\n", "        matrix_DataData_MNIST_0 = np.load(\"c:/users/kaeden/appdata/local/programs/python/python36/lib/site-packages/perceptilabs/tutorial_data/gan_mnist.npy\", mmap_mode='r+').astype(np.float32)\n", "        size_DataData_MNIST_0 = len(matrix_DataData_MNIST_0)\n", "\n", "        def generator_DataData_MNIST_0(idx_lo, idx_hi):\n", "            global matrix_DataData_MNIST_0\n", "            yield from matrix_DataData_MNIST_0[idx_lo:idx_hi]\n", "\n", "\n", "        if columns_DataData_MNIST_0 is not None:\n", "            columns[\"DataData_MNIST_0\"] = columns_DataData_MNIST_0\n", "            self._columns = columns_DataData_MNIST_0\n", "\n", "        trn_sz = int(round(0.01*70*size_DataData_MNIST_0))\n", "        val_sz = int(round(0.01*20*size_DataData_MNIST_0))\n", "        tst_sz = int(size_DataData_MNIST_0 - trn_sz - val_sz)\n", "\n", "        if trn_sz <= 0:\n", "            raise ValueError('Training dataset cannot be zero')\n", "        \n", "        if tst_sz <= 0:\n", "            raise ValueError('Testing dataset cannot be zero')\n", "\n", "        trn_sz_tot += trn_sz\n", "        val_sz_tot += val_sz\n", "        tst_sz_tot += tst_sz\n", "        \n", "        trn_gens_args_DataData_MNIST.append((generator_DataData_MNIST_0, 0, trn_sz))\n", "        val_gens_args_DataData_MNIST.append((generator_DataData_MNIST_0, trn_sz, trn_sz+val_sz))\n", "        tst_gens_args_DataData_MNIST.append((generator_DataData_MNIST_0, trn_sz+val_sz, trn_sz+val_sz+tst_sz))\n", "                    \n", "        self._trn_gens_args = trn_gens_args_DataData_MNIST\n", "        self._val_gens_args = val_gens_args_DataData_MNIST                                        \n", "        self._tst_gens_args = tst_gens_args_DataData_MNIST\n", "                    \n", "        self._trn_sz_tot = trn_sz_tot\n", "        self._val_sz_tot = val_sz_tot\n", "        self._tst_sz_tot = tst_sz_tot\n", "\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Returns any variables that the layer should make available and that can be pickled.\"\"\"\n", "        return self._variables\n", "\n", "    @property\n", "    def sample(self) -> np.ndarray:\n", "        \"\"\"Returns a single data sample\"\"\"\n", "        if self._stored_sample is None:                    \n", "            self._stored_sample = next(self.make_generator_training())\n", "\n", "        return self._stored_sample\n", "\n", "    @property\n", "    def columns(self) -> List[str]:\n", "        \"\"\"Column names. Corresponds to each column in a sample \"\"\"\n", "        return self._columns.copy()\n", "\n", "    @property\n", "    def size_training(self) -> int:\n", "        \"\"\"Returns the size of the training dataset\"\"\"                    \n", "        return self._trn_sz_tot\n", "\n", "    @property\n", "    def size_validation(self) -> int:\n", "        \"\"\"Returns the size of the validation dataset\"\"\"\n", "        return self._val_sz_tot\n", "\n", "    @property\n", "    def size_testing(self) -> int:\n", "        \"\"\"Returns the size of the testing dataset\"\"\"                    \n", "        return self._tst_sz_tot\n", "                    \n", "    def make_generator_training(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of training data.\"\"\"                                        \n", "        def gen():\n", "            for fn, lo, hi in self._trn_gens_args:\n", "                for x in fn(lo, hi):\n", "                    yield {'output': x}\n", "        return gen()\n", "        \n", "    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of validation data.\"\"\"                    \n", "        def gen():\n", "            for fn, lo, hi in self._val_gens_args:\n", "                for x in fn(lo, hi):\n", "                    yield {'output': x}                    \n", "        return gen()\n", "\n", "    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of testing data.\"\"\"                            \n", "        def gen():\n", "            for fn, lo, hi in self._tst_gens_args:\n", "                for x in fn(lo, hi):\n", "                    yield {'output': x}                                        \n", "        return gen()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ProcessReshape_Reshape_2(Tf1xLayer):\n", "    def __call__(self, inputs: Dict[str, tf.Tensor], is_training: tf.Tensor = None) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Takes a tensor as input and reshapes it.\"\"\"\n", "        \n", "                            \n", "        \n", "        shape = list((28, 28, 1))\n", "        permutation = list((0, 1, 2))\n", "        \n", "        shape = [i for i in shape if i != 0]\n", "        if(len(shape) != len(permutation)):\n", "            permutation = []\n", "            for i in range(len(shape)):\n", "                permutation.append(i)\n", "\n", "        x = inputs['input']\n", "        input_shape = x.get_shape().as_list() \n", "        new_shape = [input_shape[0] if input_shape[0] is not None else -1] + shape\n", "        \n", "        y = tf.reshape(x, new_shape)\n", "        y = tf.transpose(y, perm=[0] + [i+1 for i in permutation])\n", "\n", "            \n", "        self._outputs = {'output': y}\n", "                \n", "\n", "        return self._outputs\n", "\n", "    def get_sample(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Returns a dictionary of sample tensors\n", "\n", "        Returns:\n", "            A dictionary of sample tensors\n", "        \"\"\"\n", "        return self._outputs\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return {}\n", "\n", "    @property\n", "    def trainable_variables(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        return {}\n", "    \n", "    @property\n", "    def weights(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n", "\n", "    @property\n", "    def biases(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MathSwitch_Switch_1(Tf1xLayer):\n", "    def __init__(self):\n", "        \n", "                            \n", "        \n", "        self._selected_layer_name = '1604444597887' \n", "        self._selected_var_name = 'input1'\n", "        \n", "    def __call__(self, inputs: Dict[str, tf.Tensor], is_training: tf.Tensor = None) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Takes the outputs of all the incoming layers as input and returns the output of that layer.\"\"\"\n", "\n", "        y = inputs[self._selected_var_name]\n", "        \n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}    \n", "        self.y = y\n", "\n", "            \n", "        self._outputs = {'output': y}\n", "                \n", "\n", "        return self._outputs\n", "\n", "    def get_sample(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Returns a dictionary of sample tensors\n", "\n", "        Returns:\n", "            A dictionary of sample tensors\n", "        \"\"\"\n", "        return self._outputs\n", "\n", "    @property\n", "    def variables(self) -> Dict[str, Picklable]:\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "\n", "        return self._variables.copy()\n", "\n", "    @property\n", "    def trainable_variables(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        return {}\n", "\n", "    @property\n", "    def weights(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n", "\n", "    @property\n", "    def biases(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        return {}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepLearningFC_Dense_3(Tf1xLayer):\n", "    def __init__(self):\n", "        self._scope = 'DeepLearningFC_Dense_3'\n", "        self._n_neurons = 128\n", "        self._variables = {}\n", "        self._keep_prob = 1\n", "\n", "    def __call__(self, inputs: Dict[str, tf.Tensor], is_training: tf.Tensor = None) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor.\"\"\"\n", "        \n", "                            \n", "                \n", "        x = inputs['input']\n", "        n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)\n", "        b_norm = False\n", "        y_before = None\n", "        y = None\n", "\n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            is_training = tf.constant(True) if is_training is None else is_training\n", "            \n", "            initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)\n", "            W = tf.compat.v1.get_variable('W', initializer=initial)\n", "\n", "            initial = tf.constant(0., shape=[self._n_neurons])\n", "            b = tf.compat.v1.get_variable('b', initializer=initial)\n", "            flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)\n", "            y = tf.matmul(flat_node, W) + b\n", "\n", "            y_before = y\n", "            \n", "\n", "            y = tf.compat.v1.sigmoid(y)\n", "        \n", "\n", "            \n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "            \n", "        self._outputs = {            \n", "            'output': y,\n", "            'y_before': y_before,\n", "            'initial': tf.expand_dims(initial, axis=0),            \n", "            'W': tf.expand_dims(W, axis=0),            \n", "            'b': tf.expand_dims(b, axis=0),            \n", "            'flat_node': tf.expand_dims(flat_node, axis=0),            \n", "        }\n", "                            \n", "\n", "        return self._outputs\n", "\n", "    def get_sample(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Returns a dictionary of sample tensors\n", "\n", "        Returns:\n", "            A dictionary of sample tensors\n", "        \"\"\"\n", "        return self._outputs\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()\n", "\n", "    @property\n", "    def trainable_variables(self):\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)\n", "        variables = {v.name: v for v in variables}\n", "        return variables\n", "\n", "    @property\n", "    def weights(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            w = tf.compat.v1.get_variable('W')\n", "            return {w.name: w}\n", "\n", "    @property\n", "    def biases(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            b = tf.compat.v1.get_variable('b')\n", "            return {b.name: b}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DeepLearningFC_Is_Real(Tf1xLayer):\n", "    def __init__(self):\n", "        self._scope = 'DeepLearningFC_Is_Real'\n", "        self._n_neurons = 1\n", "        self._variables = {}\n", "        self._keep_prob = 1\n", "\n", "    def __call__(self, inputs: Dict[str, tf.Tensor], is_training: tf.Tensor = None) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Takes a tensor as input and feeds it forward through a layer of neurons, returning a newtensor.\"\"\"\n", "        \n", "                            \n", "                \n", "        x = inputs['input']\n", "        n_inputs = np.prod(x.get_shape().as_list()[1:], dtype=np.int32)\n", "        b_norm = False\n", "        y_before = None\n", "        y = None\n", "\n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            is_training = tf.constant(True) if is_training is None else is_training\n", "            \n", "            initial = tf.random.truncated_normal((n_inputs, self._n_neurons), stddev=0.1)\n", "            W = tf.compat.v1.get_variable('W', initializer=initial)\n", "\n", "            initial = tf.constant(0., shape=[self._n_neurons])\n", "            b = tf.compat.v1.get_variable('b', initializer=initial)\n", "            flat_node = tf.cast(tf.reshape(x, [-1, n_inputs]), dtype=tf.float32)\n", "            y = tf.matmul(flat_node, W) + b\n", "\n", "            y_before = y\n", "            \n", "\n", "                            \n", "\n", "            \n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "\n", "            \n", "        self._outputs = {            \n", "            'output': y,\n", "            'y_before': y_before,\n", "            'initial': tf.expand_dims(initial, axis=0),            \n", "            'W': tf.expand_dims(W, axis=0),            \n", "            'b': tf.expand_dims(b, axis=0),            \n", "            'flat_node': tf.expand_dims(flat_node, axis=0),            \n", "        }\n", "                            \n", "\n", "        return self._outputs\n", "\n", "    def get_sample(self) -> Dict[str, tf.Tensor]:\n", "        \"\"\" Returns a dictionary of sample tensors\n", "\n", "        Returns:\n", "            A dictionary of sample tensors\n", "        \"\"\"\n", "        return self._outputs\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()\n", "\n", "    @property\n", "    def trainable_variables(self):\n", "        \"\"\"Any trainable variables belonging to this layer that should be updated during backpropagation. Their gradients will also be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"\n", "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._scope)\n", "        variables = {v.name: v for v in variables}\n", "        return variables\n", "\n", "    @property\n", "    def weights(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            w = tf.compat.v1.get_variable('W')\n", "            return {w.name: w}\n", "\n", "    @property\n", "    def biases(self):\n", "        \"\"\"Any weight tensors belonging to this layer that should be rendered in the frontend.\n", "\n", "        Return:\n", "            A dictionary with tensor names for keys and tensors for values.\n", "        \"\"\"        \n", "        with tf.compat.v1.variable_scope(self._scope, reuse=tf.compat.v1.AUTO_REUSE):\n", "            b = tf.compat.v1.get_variable('b')\n", "            return {b.name: b}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["################################################### Main #####################################################\n", "\n", "\n", "class TrainGan_GAN_1(GANLayer):\n", "\n", "    def __init__(self):\n", "        \n", "                            \n", "                \n", "        self._n_epochs = 2\n", "        self._batch_size = 100\n", "\n", "        self._target_acc = 0\n", "        self._stop_condition = 'Epochs'\n", "        self._stopped = False\n", "        self._paused = False\n", "        self._headless = False\n", "        self._status = 'created'\n", "        \n", "        self._generator_loss_training = 0.0\n", "        self._generator_loss_validation = 0.0\n", "        self._generator_loss_testing = 0.0   \n", "\n", "        self._discriminator_loss_training = 0.0\n", "        self._discriminator_loss_validation = 0.0\n", "        self._discriminator_loss_testing = 0.0   \n", "\n", "        self._variables = {}\n", "        self._generator_variables = {}\n", "        self._discriminator_variables = {}\n", "\n", "        self._real_layer_outputs = {}\n", "        self._generator_layer_outputs = {}\n", "        self._generator_layer_weights = {}\n", "        self._generator_layer_biases = {}        \n", "        self._generator_layer_gradients = {}\n", "\n", "        self._random_discriminator_layer_outputs = {}\n", "        self._real_discriminator_layer_outputs = {}\n", "        self._discriminator_layer_outputs = {}\n", "        self._discriminator_layer_weights = {}\n", "        self._discriminator_layer_biases = {}        \n", "        self._discriminator_layer_gradients = {}\n", "\n", "        self._layer_outputs = {}\n", "        self._layer_weights = {}\n", "        self._layer_biases = {}        \n", "        self._layer_gradients = {}\n", "\n", "        self._training_iteration = 0\n", "        self._validation_iteration = 0\n", "        self._testing_iteration = 0\n", "\n", "        self._trn_sz_tot = 0\n", "        self._val_sz_tot = 0\n", "        self._tst_sz_tot = 0        \n", "        \n", "        self._random_means = []\n", "        self._random_stds = []\n", "        self._real_means = []\n", "        self._real_stds = []\n", "\n", "        self._checkpoint = None\n", "\n", "    def init_layer(self, graph: Graph, mode = 'initializing'):\n", "        \"\"\"This is the function that makes the training layer runnable. We take all variable initializations for tensors and initializers and wrap them in dictionaries\n", "        to be called in run().\n", "        \"\"\"\n", "        self._mode = mode\n", "        if tf.compat.v1.get_default_session() is not None:\n", "            tf.compat.v1.get_default_session().close() \n", "            tf.reset_default_graph()\n", "        self._status = 'initializing'\n", "\n", "        real_data_layer_id = \"DataData_MNIST\"\n", "\n", "        switch_layer_id = \"MathSwitch_Switch_1\"\n", "\n", "        for node in graph.data_nodes:\n", "            if node.layer_id != real_data_layer_id and not node.is_training_node:\n", "                random_data_node = node\n", "            elif node.layer_id == real_data_layer_id:\n", "                real_data_node = node\n", "\n", "        training_node = graph.nodes[-1]\n", "        switch_node = graph.get_node_by_id(switch_layer_id)\n", "        \n", "        output_node = [node for node in graph.get_input_nodes(training_node)][0]\n", "        output_layer_id = output_node.layer_id\n", "        self._switch_layer_id = switch_layer_id\n", "\n", "        self._selected_layer_id = \"1604444597887\"\n", "        \n", "        generator_nodes = graph.get_nodes_inbetween(random_data_node, switch_node)\n", "        discriminator_nodes = graph.get_nodes_inbetween(switch_node, output_node)\n", "        real_nodes = graph.get_nodes_inbetween(real_data_node, switch_node)\n", "\n", "        generator_layer_ids = [node.layer_id for node in generator_nodes]\n", "        discriminator_layer_ids = [node.layer_id for node in discriminator_nodes]\n", "\n", "        self._generator_layer_ids = generator_layer_ids\n", "\n", "        self._trn_sz_tot = real_data_node.layer.size_training\n", "        self._val_sz_tot = real_data_node.layer.size_validation\n", "        self._tst_sz_tot = real_data_node.layer.size_testing\n", "\n", "        real_data_sample = real_data_node.layer_instance.sample\n", "        random_data_sample = random_data_node.layer_instance.sample \n", "        # Make training set\n", "        dataset_trn = tf.data.Dataset.zip((\n", "            tf.data.Dataset.from_generator(\n", "                real_data_node.layer_instance.make_generator_training,\n", "                output_shapes={k: v.shape for k, v in real_data_sample.items()},\n", "                output_types={k: v.dtype for k, v in real_data_sample.items()}                \n", "            ),\n", "            tf.data.Dataset.from_generator(\n", "                random_data_node.layer_instance.make_generator_training,\n", "                output_shapes={k: v.shape for k, v in random_data_sample.items()},\n", "                output_types={k: v.dtype for k, v in random_data_sample.items()}\n", "            )\n", "        ))\n", "\n", "        # Make validation set\n", "        dataset_val = tf.data.Dataset.zip((\n", "            tf.data.Dataset.from_generator(\n", "                real_data_node.layer_instance.make_generator_validation,\n", "                output_shapes={k: v.shape for k, v in real_data_sample.items()},\n", "                output_types={k: v.dtype for k, v in real_data_sample.items()}                \n", "            ),\n", "            tf.data.Dataset.from_generator(\n", "                random_data_node.layer_instance.make_generator_validation,\n", "                output_shapes={k: v.shape for k, v in random_data_sample.items()},\n", "                output_types={k: v.dtype for k, v in random_data_sample.items()}\n", "            )\n", "        ))\n", "\n", "        # Make testing set\n", "        dataset_tst = tf.data.Dataset.zip((\n", "            tf.data.Dataset.from_generator(\n", "                real_data_node.layer_instance.make_generator_testing,\n", "                output_shapes={k: v.shape for k, v in real_data_sample.items()},\n", "                output_types={k: v.dtype for k, v in real_data_sample.items()}             \n", "            ),\n", "            tf.data.Dataset.from_generator(\n", "                random_data_node.layer_instance.make_generator_testing,\n", "                output_shapes={k: v.shape for k, v in random_data_sample.items()},\n", "                output_types={k: v.dtype for k, v in random_data_sample.items()}\n", "            )\n", "        ))\n", "\n", "        dataset_trn = dataset_trn.batch(self._batch_size)\n", "        dataset_val = dataset_val.batch(self._batch_size)\n", "        dataset_tst = dataset_tst.batch(1)\n", "\n", "        # Make initializers\n", "        iterator = tf.data.Iterator.from_structure(dataset_trn.output_types, dataset_trn.output_shapes)\n", "        trn_init = iterator.make_initializer(dataset_trn)\n", "        val_init = iterator.make_initializer(dataset_val)\n", "        tst_init = iterator.make_initializer(dataset_tst)        \n", "        real_tensor , random_tensor = iterator.get_next()\n", "\n", "        \n", "\n", "        # Build the TensorFlow graph\n", "        def build_real_graph(real_tensor):\n", "            if len(real_nodes) > 1:\n", "                switch_node.layer._selected_layer_id = real_nodes[-2].layer_id\n", "                switch_node.layer._selected_var_name = [dst_var for src_node, src_var, dst_var in graph.get_input_connections(switch_node) if src_node.layer_id == switch_node.layer._selected_layer_id][0]\n", "            else:\n", "                switch_node.layer._selected_layer_id = real_data_node.layer_id\n", "                switch_node.layer._selected_var_name = [dst_var for src_node, src_var, dst_var in graph.get_input_connections(switch_node) if src_node.layer_id == switch_node.layer._selected_layer_id][0]\n", "\n", "            _random_data_node=random_data_node\n", "\n", "            layer_output_tensors = {\n", "                real_data_node.layer_id: real_tensor\n", "            }\n", "\n", "            for dst_node in real_nodes:\n", "                inputs = {\n", "                    dst_var: layer_output_tensors[src_node.layer_id][src_var]\n", "                    for src_node, src_var, dst_var in graph.get_input_connections(dst_node)\n", "                    if src_node not in generator_nodes + [_random_data_node]\n", "                }\n", "                y = dst_node.layer_instance(\n", "                    inputs\n", "                )\n", "                layer_output_tensors[dst_node.layer_id] = y\n", "\n", "            return layer_output_tensors\n", "\n", "        def build_generator_graph(random_tensor):\n", "            switch_node.layer._selected_layer_id = generator_nodes[-2].layer_id\n", "            switch_node.layer._selected_var_name = [dst_var for src_node, src_var, dst_var in graph.get_input_connections(switch_node) if src_node.layer_id == switch_node.layer._selected_layer_id][0]\n", "            _real_data_node = real_data_node\n", "\n", "            layer_output_tensors = {\n", "                random_data_node.layer_id: random_tensor\n", "            }\n", "\n", "            for dst_node in generator_nodes:\n", "                inputs = {\n", "                    dst_var: layer_output_tensors[src_node.layer_id][src_var]\n", "                    for src_node, src_var, dst_var in graph.get_input_connections(dst_node) \n", "                    if src_node not in real_nodes+[_real_data_node]\n", "                }\n", "                y = dst_node.layer_instance(\n", "                    inputs\n", "                )\n", "                layer_output_tensors[dst_node.layer_id] = y\n", "\n", "\n", "\n", "            # for node in generator_nodes:\n", "            #     if len(list(graph.get_input_nodes(node))) <= 1:\n", "            #         args = []\n", "            #         for input_node in graph.get_input_nodes(node):\n", "            #             args.append(layer_output_tensors[input_node.layer_id])\n", "            #         y = node.layer_instance(*args)\n", "            #     elif len(list(graph.get_input_nodes(node))) > 1:\n", "            #         args = {}\n", "            #         for input_node in graph.get_input_nodes(node):\n", "            #             if input_node in generator_nodes:\n", "            #                 args[input_node.layer_id] = layer_output_tensors[input_node.layer_id]\n", "            #         y = node.layer_instance(args)\n", "            #     layer_output_tensors[node.layer_id] = y\n", "            return layer_output_tensors\n", "        \n", "        def build_discriminator_graph(input_tensor):\n", "            layer_output_tensors = {\n", "                switch_node.layer_id: input_tensor\n", "            }\n", "            for dst_node in discriminator_nodes:\n", "                inputs = {\n", "                    dst_var: layer_output_tensors[src_node.layer_id][src_var]\n", "                    for src_node, src_var, dst_var in graph.get_input_connections(dst_node)\n", "                }\n", "                y = dst_node.layer_instance(\n", "                    inputs\n", "                )\n", "                layer_output_tensors[dst_node.layer_id] = y\n", "\n", "\n", "            return layer_output_tensors\n", "\n", "        generator_layer_output_tensors = build_generator_graph(random_tensor)\n", "        real_layer_output_tensors = build_real_graph(real_tensor)\n", "        \n", "        real_output_tensor = real_layer_output_tensors[switch_layer_id]\n", "        generator_output_tensor = generator_layer_output_tensors[switch_layer_id]\n", "        \n", "        real_discriminator_layer_output_tensors = build_discriminator_graph(real_output_tensor)\n", "        random_discriminator_layer_output_tensors = build_discriminator_graph(generator_output_tensor)\n", "        \n", "        # real_discriminator_outputs = real_discriminator_layer_output_tensors[output_layer_id]\n", "        # random_discriminator_outputs = random_discriminator_layer_output_tensors[output_layer_id]\n", "        \n", "        # Create an exportable version of the TensorFlow graph\n", "        \n", "        # self._real_tensor_export = tf.placeholder(shape=dataset_trn.output_shapes[0], dtype=dataset_trn.output_types[0])\n", "        # self._random_tensor_export = tf.placeholder(shape=dataset_trn.output_shapes[1], dtype=dataset_trn.output_types[1])\n", "\n", "        self._real_tensor_export= {\n", "            key: tf.placeholder(shape=shape, dtype=type_)\n", "            for (key, shape), (_, type_) in zip(dataset_trn.output_shapes[0].items(), dataset_trn.output_types[0].items())            \n", "        }\n", "        self._random_tensor_export= {\n", "            key: tf.placeholder(shape=shape, dtype=type_)\n", "            for (key, shape), (_, type_) in zip(dataset_trn.output_shapes[1].items(), dataset_trn.output_types[1].items())            \n", "        }\n", "        \n", "\n", "        self._real_output_tensor_export = build_real_graph(self._real_tensor_export)[switch_layer_id]\n", "        self._generator_output_tensor_export = build_generator_graph(self._random_tensor_export)[switch_layer_id]\n", "\n", "        self._discriminator_real_output_tensor_export = build_discriminator_graph(self._real_output_tensor_export)[output_layer_id]\n", "        self._discriminator_random_output_tensor_export = build_discriminator_graph(self._generator_output_tensor_export)[output_layer_id]\n", "        \n", "        #loss function\n", "\n", "        def loss_func(logits_in,labels_in):\n", "            return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_in,labels=labels_in))\n", "\n", "        real_discriminator_output_tensor = None\n", "        random_discriminator_output_tensor = None\n", "        \n", "        for src_node, src_var, dst_var in graph.get_input_connections(graph.active_training_node):\n", "            if dst_var == 'input':\n", "                real_discriminator_output_tensor = real_discriminator_layer_output_tensors[output_layer_id][src_var]\n", "                random_discriminator_output_tensor = random_discriminator_layer_output_tensors[output_layer_id][src_var]\n", "        \n", "        discriminator_real_loss=loss_func(real_discriminator_output_tensor,tf.ones_like(real_discriminator_output_tensor)*0.9) \n", "        discriminator_random_loss=loss_func(random_discriminator_output_tensor,tf.zeros_like(random_discriminator_output_tensor))\n", "        \n", "        discriminator_loss_tensor = discriminator_real_loss+discriminator_random_loss\n", "        generator_loss_tensor = loss_func(random_discriminator_output_tensor,tf.ones_like(random_discriminator_output_tensor))\n", "\n", "        global_step = None\n", "\n", "        generator_optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n", "\n", "\n", "        discriminator_optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n", "\n", "        generator_layer_weight_tensors = {}\n", "        generator_layer_bias_tensors = {}        \n", "        generator_layer_gradient_tensors = {}\n", "\n", "        for node in generator_nodes+real_nodes:\n", "            if not isinstance(node.layer, Tf1xLayer): # In case of pure custom layers...\n", "                continue\n", "            \n", "            generator_layer_weight_tensors[node.layer_id] = node.layer.weights\n", "            generator_layer_bias_tensors[node.layer_id] = node.layer.biases\n", "            \n", "            if len(node.layer.trainable_variables) > 0:\n", "                gradients = {}\n", "                for name, tensor in node.layer.trainable_variables.items():\n", "                    grad_tensor = tf.gradients(generator_loss_tensor, tensor)\n", "                    if any(x is None for x in grad_tensor):\n", "                        grad_tensor = tf.constant(0)\n", "                    gradients[name] = grad_tensor\n", "                generator_layer_gradient_tensors[node.layer_id] = gradients\n", "\n", "        discriminator_layer_weight_tensors = {}\n", "        discriminator_layer_bias_tensors = {}        \n", "        discriminator_layer_gradient_tensors = {}\n", "\n", "        for node in discriminator_nodes:\n", "            if not isinstance(node.layer, Tf1xLayer): # In case of pure custom layers...\n", "                continue\n", "            \n", "            discriminator_layer_weight_tensors[node.layer_id] = node.layer.weights\n", "            discriminator_layer_bias_tensors[node.layer_id] = node.layer.biases\n", "            \n", "            if len(node.layer.trainable_variables) > 0:\n", "                gradients = {}\n", "                for name, tensor in node.layer.trainable_variables.items():\n", "                    grad_tensor = tf.gradients(discriminator_loss_tensor, tensor)\n", "                    if any(x is None for x in grad_tensor):\n", "                        grad_tensor = tf.constant(0)\n", "                    gradients[name] = grad_tensor\n", "                discriminator_layer_gradient_tensors[node.layer_id] = gradients\n", "\n", "        \n", "        trainable_vars = tf.trainable_variables()\n", "        \n", "        discriminator_vars=[]\n", "        for var in trainable_vars:\n", "            for discriminator_layer_id in discriminator_layer_ids:\n", "                if discriminator_layer_id in var.name:\n", "                    discriminator_vars.append(var)\n", "\n", "        discriminator_update_weights = discriminator_optimizer.minimize(discriminator_loss_tensor, var_list = discriminator_vars, global_step=global_step)\n", "\n", "        generator_vars=[]\n", "        for var in trainable_vars:\n", "            for generator_layer_id in generator_layer_ids:\n", "                if generator_layer_id in var.name:\n", "                    generator_vars.append(var)\n", "\n", "        generator_update_weights = generator_optimizer.minimize(generator_loss_tensor, var_list = generator_vars, global_step=global_step) \n", "             \n", "\n", "        sess = None\n", "         \n", "        config = tf.ConfigProto(device_count={\"GPU\": 0})\n", "        sess = tf.Session(config=config)\n", "      \n", "        self._sess = sess\n", "        \n", "        trackable_variables = {}\n", "        trackable_variables.update({x.name: x for x in tf.trainable_variables() if isinstance(x, Trackable)})\n", "        trackable_variables.update({k: v for k, v in locals().items() if isinstance(v, Trackable) and not isinstance(v, tf.python.data.ops.iterator_ops.Iterator)})\n", "        self._checkpoint = tf.train.Checkpoint(**trackable_variables)\n", "        sess.run(tf.global_variables_initializer())\n", "        \n", "        checkpoint_directory = '//192.168.1.31/UserFiles/Kaeden/My Documents/Perceptilabs/Default/GAN_Test/checkpoint'\n", "        use_checkpoint = False\n", "        if use_checkpoint:\n", "            path = tf.train.latest_checkpoint(checkpoint_directory)\n", "            if path is not None:\n", "                status = self._checkpoint.restore(path)\n", "                status.run_restore_ops(session=self._sess)\n", "            elif path is None and self._mode == 'testing':\n", "                log.error('There are no saved checkpoint files for this model.')\n", "                self._sess.close()\n", "\n", "\n", "        self._discriminator_layer_gradient = discriminator_layer_gradient_tensors\n", "        self._discriminator_layer_bias = discriminator_layer_bias_tensors\n", "        self._discriminator_layer_weight = discriminator_layer_weight_tensors.copy()\n", "        self._real_discriminator_layer_output = real_discriminator_layer_output_tensors\n", "        self._random_discriminator_layer_output = random_discriminator_layer_output_tensors\n", "        self._discriminator_loss = discriminator_loss_tensor\n", "        self._discriminator_weight = discriminator_update_weights\n", "        self._real_layer_output = real_layer_output_tensors\n", "        self._gen_layer_gradient = generator_layer_gradient_tensors\n", "        self._gen_layer_bias = generator_layer_bias_tensors\n", "        self._gen_layer_weight = generator_layer_weight_tensors.copy()\n", "        self._gen_layer_output = generator_layer_output_tensors.copy()\n", "        self._gen_loss = generator_loss_tensor\n", "        self._gen_weight = generator_update_weights\n", "\n", "        self._trn_init = trn_init\n", "        self._val_init = val_init\n", "        self._tst_init = tst_init\n", "\n", "        \n", "    def train(self, graph:Graph):\n", "        \"\"\"Training is done when this function is called. Once the training ends, checkpoint files are saved.\n", "        \"\"\"\n", "    \n", "        sess = self._sess\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "        self._checkpoint_save_path = '//192.168.1.31/UserFiles/Kaeden/My Documents/Perceptilabs/Default/GAN_Test/checkpoint'\n", "\n", "\n", "        def train_step(sess):\n", "            if not self._headless:\n", "                _, self._generator_loss_training, self._generator_layer_outputs,\\\n", "                    self._generator_layer_weights, self._generator_layer_biases, \\\n", "                    self._generator_layer_gradients, self._real_layer_outputs, \\\n", "                _, self._discriminator_loss_training, self._random_discriminator_layer_outputs, self._real_discriminator_layer_outputs,\\\n", "                    self._discriminator_layer_weights, self._discriminator_layer_biases, \\\n", "                    self._discriminator_layer_gradients \\\n", "                    = sess.run([\n", "                        self._gen_weight, self._gen_loss, self._gen_layer_output, \\\n", "                        self._gen_layer_weight, self._gen_layer_bias, self._gen_layer_gradient, self._real_layer_output, \\\n", "                        self._discriminator_weight, self._discriminator_loss, self._random_discriminator_layer_output, \\\n", "                        self._real_discriminator_layer_output, self._discriminator_layer_weight, self._discriminator_layer_bias, self._discriminator_layer_gradient\n", "                    ])\n", "                \n", "            else:\n", "                _, self._generator_loss_training, \\\n", "                    _, self.__discriminator_loss_training \\\n", "                    = sess.run([\n", "                        self._gen_weight, self._gen_loss, \\\n", "                            self._discriminator_weight, self._discriminator_loss\n", "                    ])\n", "\n", "        def validation_step(sess):\n", "            if not self._headless:\n", "                self._generator_loss_validation, self._generator_layer_outputs,\\\n", "                    self._generator_layer_weights, self._generator_layer_biases, \\\n", "                    self._generator_layer_gradients, self._real_layer_outputs, \\\n", "                    self._discriminator_loss_validation, self._discriminator_layer_outputs,\\\n", "                    self._discriminator_layer_weights, self._discriminator_layer_biases, \\\n", "                    self._discriminator_layer_gradients \\\n", "                    = sess.run([\n", "                        self._gen_loss, self._gen_layer_output, \\\n", "                        self._gen_layer_weight, self._gen_layer_bias, self._gen_layer_gradient, self._real_layer_output, \\\n", "                        self._discriminator_loss, self._random_discriminator_layer_output, \\\n", "                        self._discriminator_layer_weight, self._discriminator_layer_bias, self._discriminator_layer_gradient\n", "                    ])\n", "\n", "            else:\n", "                self._generator_loss_validation, \\\n", "                    self.__discriminator_loss_validation \\\n", "                    = sess.run(\n", "                        self._gen_loss, \\\n", "                        self._discriminator_loss\n", "                    )\n", "\n", "        log.info(\"Entering training loop\")\n", "\n", "        # Training loop\n", "        self._epoch = 0\n", "        while self._epoch < self._n_epochs and not self._stopped:\n", "            t0 = time.perf_counter()\n", "            self._training_iteration = 0\n", "            self._validation_iteration = 0\n", "            self._status = 'training'\n", "            sess.run(self._trn_init)            \n", "            try:\n", "                while not self._stopped:\n", "                    train_step(sess)\n", "                    yield YieldLevel.SNAPSHOT\n", "                    self._training_iteration += 1\n", "            except tf.errors.OutOfRangeError:\n", "                pass\n", "\n", "            self._status = 'validation'\n", "            sess.run(self._val_init)            \n", "            try:\n", "                while not self._stopped:\n", "                    validation_step(sess)\n", "                    yield YieldLevel.SNAPSHOT                    \n", "                    self._validation_iteration += 1\n", "            except tf.errors.OutOfRangeError:\n", "                pass\n", "            log.info(\n", "                f\"Finished epoch {self._epoch+1}/{self._n_epochs} - \"\n", "                f\"generator loss training, validation: {self.generator_loss_training:.6f}, {self.generator_loss_validation:.6f} - \"\n", "                f\"discriminator loss training, validation: {self.discriminator_loss_training:.6f}, {self.discriminator_loss_validation:.6f} - \"\n", "            )\n", "            log.info(f\"Epoch duration: {round(time.perf_counter() - t0, 3)} s\")  \n", "            if self._stop_condition == \"TargetAccuracy\" and self._accuracy_training * 100 >= self._target_acc:\n", "                break          \n", "            self._epoch += 1\n", "\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}            \n", "        self._status = 'finished'\n", "        yield YieldLevel.SNAPSHOT\n", "        self.on_export(self._checkpoint_save_path, 'checkpoint')   \n", "        sess.close()\n", "\n", " \n", "\n", "    def test(self, graph:Graph):\n", "        \"\"\"Testing is done when this function is called. \n", "        \"\"\"\n", "\n", "    \n", "        \n", "        sess = self._sess\n", "        self._epoch = 0\n", "\n", "        def test_step(sess):\n", "            self._generator_loss_testing, self._generator_layer_outputs,\\\n", "                self._generator_layer_weights, self._generator_layer_gradients, self._real_layer_outputs, \\\n", "                self._discriminator_loss_testing, self._discriminator_layer_outputs,\\\n", "                self._discriminator_layer_weights, self._discriminator_layer_gradients \\\n", "                = sess.run([\n", "                    self._gen_loss, self._gen_layer_output, \\\n", "                    self._gen_layer_weight, self._gen_layer_gradient, self._real_layer_output, \\\n", "                    self._discriminator_loss, self._random_discriminator_layer_output, \\\n", "                    self._discriminator_layer_weight, self._discriminator_layer_gradient\n", "                ])\n", "\n", "        # Test loop\n", "        log.info(\"Entering testing loop\")\n", "        self._status = 'testing'\n", "\n", "        self._testing_iteration = 0\n", "        sess.run(self._tst_init)                                \n", "        while not self._stopped:\n", "            try:\n", "                test_step(sess)\n", "                yield YieldLevel.SNAPSHOT\n", "                self._testing_iteration += 1\n", "            except tf.errors.OutOfRangeError:\n", "                self._testing_iteration = 0\n", "                sess.run(self._tst_init)\n", "                test_step(sess)   \n", "                yield YieldLevel.SNAPSHOT\n", "        \n", "        self._status = 'finished'\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)}\n", "        yield YieldLevel.SNAPSHOT\n", "        sess.close()\n", "\n", " \n", "\n", "    def run(self, graph: Graph, mode = 'initializing'):\n", "        \"\"\"Called as the main entry point for training. Responsible for training the model.\n", "\n", "        Args:\n", "            graph: A PerceptiLabs Graph object containing references to all layers objects included in the model produced by this training layer.\n", "        \"\"\"  \n", "        self.init_layer(graph, mode)\n", "        self._variables = {k: v for k, v in locals().items() if can_serialize(v)} \n", "        \n", "        if mode == 'training':\n", "            yield from self.train(graph)\n", "        elif mode == 'testing':\n", "            yield from self.test(graph)\n", "\n", "\n", "    def on_export(self, path: str, mode: str) -> None:\n", "        \"\"\"Called when the export button is clicked in the frontend.\n", "        It is up to the implementing layer to save the model to disk.\n", "        \n", "        Args:\n", "            path: the directory where the exported model will be stored.\n", "            mode: how to export the model. Made available to frontend via 'export_modes' property.\"\"\"\n", "\n", "        log.debug(f\"Export called. Project path = {path}, mode = {mode}\")\n", "        \n", "        if mode in ['TFModel', 'TFLite']:\n", "            pb_path = os.path.join(path, '1')\n", "            if os.path.exists(pb_path):\n", "                shutil.rmtree(pb_path)\n", "            \n", "            time.sleep(.0000000000000001) #Force your computer to do a clock cycle to avoid Windows permission exception\n", "\n", "            os.makedirs(pb_path, exist_ok=True)\n", "        \n", "        # Export non-compressed model\n", "        if mode in ['TFModel']:\n", "            tf.compat.v1.saved_model.simple_save(self._sess, pb_path, \\\n", "                inputs={'real_input': self._real_tensor_export['output'], 'random_input': self._random_tensor_export['output']}, \\\n", "                outputs={'real_output': self._discriminator_real_output_tensor_export['output'], 'random_output': self._discriminator_random_output_tensor_export['output'] })\n", "\n", "        # Export compressed model\n", "        if mode in ['TFLite']:\n", "            frozen_path = os.path.join(pb_path, 'frozen_model.pb')\n", "            converter = tf.lite.TFLiteConverter.from_session(self._sess, [self._real_tensor_export['output'], self._random_tensor_export['output']], [self._discriminator_real_output_tensor_export['output'], self._discriminator_random_output_tensor_export['output']])\n", "            converter.post_training_quantize = True\n", "            tflite_model = converter.convert()\n", "            with open(frozen_path, \"wb\") as f:\n", "                f.write(tflite_model)\n", "\n", "        # Export checkpoint\n", "        if mode in ['checkpoint']:\n", "            for fname in os.listdir(path):\n", "                if fname.endswith('.json'):\n", "                    pass\n", "                else:\n", "                    os.remove(os.path.join(path,fname))\n", "            self._checkpoint.save(file_prefix=os.path.join(path, 'model.ckpt'), session=self._sess)\n", "                \n", "    def on_stop(self) -> None:\n", "        \"\"\"Called when the save model button is clicked in the frontend. \n", "        It is up to the implementing layer to save the model to disk.\"\"\"\n", "        self.on_export(self._checkpoint_save_path, 'checkpoint')   \n", "        self._stopped = True\n", "\n", "    def on_headless_activate(self) -> None:\n", "        \"\"\"\"Called when the statistics shown in statistics window are not needed.\n", "        Purose is to speed up the iteration speed significantly.\"\"\"\n", "        self._headless = True\n", "\n", "        self._layer_outputs = {} \n", "        self._layer_weights = {}\n", "        self._layer_biases = {}\n", "        self._layer_gradients = {}\n", "\n", "    def on_headless_deactivate(self) -> None:\n", "        \"\"\"\"Called when the statistics shown in statistics window are needed.\n", "        May slow down the iteration speed of the training.\"\"\"\n", "        import time\n", "        log.info(f\"Set to headless_off at time {time.time()}\")\n", "        self._headless = False\n", "\n", "    @property\n", "    def export_modes(self) -> List[str]:\n", "        \"\"\"Returns the possible modes of exporting a model.\"\"\"        \n", "        return [\n", "            'TFModel',\n", "            'TFLite'\n", "            'TFModel+checkpoint',\n", "            'TFLite+checkpoint',            \n", "        ]\n", "        \n", "    @property\n", "    def is_paused(self) -> None:\n", "        \"\"\"Returns true when the training is paused.\"\"\"        \n", "        return self._paused\n", "\n", "    @property\n", "    def batch_size(self):\n", "        \"\"\" Size of the current training batch \"\"\"        \n", "        return self._batch_size\n", "\n", "    @property\n", "    def status(self):\n", "        \"\"\"Called when the pause button is clicked in the frontend. It is up to the implementing layer to pause its execution.\"\"\"        \n", "        return self._status\n", "    \n", "    @property\n", "    def epoch(self):\n", "        \"\"\"The current epoch\"\"\"        \n", "        return self._epoch\n", "\n", "    @property\n", "    def variables(self):\n", "        \"\"\"Any variables belonging to this layer that should be rendered in the frontend.\n", "        \n", "        Returns:\n", "            A dictionary with tensor names for keys and picklable for values.\n", "        \"\"\"\n", "        return self._variables.copy()        \n", "\n", "    @property\n", "    def sample(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"Returns a single data sample\"\"\"       \n", "        sample = np.array([self._generator_loss_training])\n", "        return {'output': sample}\n", "\n", "    @property\n", "    def columns(self) -> List[str]: \n", "        \"\"\"Column names. Corresponds to each column in a sample \"\"\"\n", "        return []\n", "    \n", "    @property\n", "    def size_training(self) -> int:\n", "        \"\"\"Returns the size of the training dataset\"\"\"                                    \n", "        return self._trn_sz_tot\n", "\n", "    @property\n", "    def size_validation(self) -> int:\n", "        \"\"\"Returns the size of the validation dataset\"\"\"                                            \n", "        return self._val_sz_tot\n", "\n", "    @property\n", "    def size_testing(self) -> int:\n", "        \"\"\"Returns the size of the testing dataset\"\"\"\n", "        return self._tst_sz_tot\n", "\n", "    def make_generator_training(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of training data. In the case of a training layer, this typically yields the model output.\"\"\"        \n", "        yield from []\n", "        \n", "    def make_generator_validation(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of validation data. In the case of a training layer, this typically yields the model output.\"\"\"                \n", "        yield from []\n", "        \n", "    def make_generator_testing(self) -> Generator[np.ndarray, None, None]:\n", "        \"\"\"Returns a generator yielding single samples of testing data. In the case of a training layer, this typically yields the model output.\"\"\"                        \n", "        yield from []\n", "\n", "\n", "    @property\n", "    def get_switch_layer_id(self):\n", "        \"\"\" Returns the layer name of the Switch layer\"\"\"\n", "        return self._switch_layer_id\n", "    \n", "    @property\n", "    def generator_loss_training(self) -> float:\n", "        \"\"\"Returns the current loss of the generator training phase\"\"\"                \n", "        return self._generator_loss_training        \n", "\n", "    @property\n", "    def generator_loss_validation(self) -> float:\n", "        \"\"\"Returns the current loss of the generator validation phase\"\"\"                        \n", "        return self._generator_loss_validation        \n", "\n", "    @property\n", "    def generator_loss_testing(self) -> float:\n", "        \"\"\"Returns the current loss of the generator testing phase\"\"\"                \n", "        return self._generator_loss_testing\n", "    \n", "    @property\n", "    def discriminator_loss_validation(self) -> float:\n", "        \"\"\"Returns the current loss of the discriminator validation phase\"\"\"                        \n", "        return self._discriminator_loss_validation        \n", "\n", "    @property\n", "    def discriminator_loss_training(self) -> float:\n", "        \"\"\"Returns the current loss of the discriminator testing phase\"\"\"                \n", "        return self._discriminator_loss_training\n", "\n", "    @property\n", "    def discriminator_loss_testing(self) -> float:\n", "        \"\"\"Returns the current loss of the discriminator testing phase\"\"\"                \n", "        return self._discriminator_loss_testing\n", "\n", "    @property\n", "    def layer_weights(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The weight values of each layer in the input Graph during the training.\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.\n", "        \"\"\"      \n", "        self._layer_weights.update(self._generator_layer_weights)\n", "        self._layer_weights.update(self._discriminator_layer_weights)   \n", "        return self._layer_weights\n", "\n", "    @property\n", "    def layer_biases(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The bias values of each layer in the input Graph during the training.\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain weight name and value pairs. The values must be picklable.\n", "        \"\"\"       \n", "        self._layer_biases.update(self._generator_layer_biases)\n", "        self._layer_biases.update(self._discriminator_layer_biases) \n", "        return self._layer_biases\n", "    \n", "    @property\n", "    def layer_gradients(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The gradients with respect to the loss of all trainable variables of each layer in the input Graph.\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain gradient name and value pairs. The values must be picklable.\n", "        \"\"\"\n", "        self._layer_gradients.update(self._generator_layer_gradients)\n", "        self._layer_gradients.update(self._discriminator_layer_gradients)       \n", "        return self._layer_gradients\n", "    \n", "    @property\n", "    def layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.\n", "        \"\"\"\n", "        if self._selected_layer_id in self._generator_layer_ids:\n", "            self._layer_outputs.update(self._real_layer_outputs)\n", "            self._layer_outputs.update(self._generator_layer_outputs)\n", "            self._layer_outputs.update(self._random_discriminator_layer_outputs)\n", "        else:\n", "            self._layer_outputs.update(self._generator_layer_outputs)\n", "            self._layer_outputs.update(self._real_layer_outputs)\n", "            self._layer_outputs.update(self._real_discriminator_layer_outputs)\n", "        return self._layer_outputs\n", "\n", "    @property\n", "    def generator_layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.\n", "        \"\"\"\n", "        return self._generator_layer_outputs\n", "    \n", "    @property\n", "    def real_layer_outputs(self) -> Dict[str, Dict[str, Picklable]]:\n", "        \"\"\"The output values of each layer in the input Graph during the training (e.g., tf.Tensors evaluated for each iteration)\n", "\n", "        Returns:\n", "            A dictionary of nested dictionaries, where each key is a layer id. The nested dictionaries contain variable name and value pairs. The values must be picklable.\n", "        \"\"\"\n", "        return self._real_layer_outputs\n", "\n", "    @property\n", "    def training_iteration(self) -> int:\n", "        \"\"\"The current training iteration\"\"\"\n", "        return self._training_iteration\n", "\n", "    @property\n", "    def validation_iteration(self) -> int:\n", "        \"\"\"The current validation iteration\"\"\"        \n", "        return self._validation_iteration\n", "\n", "    @property\n", "    def testing_iteration(self) -> int:\n", "        \"\"\"The current testing iteration\"\"\"                \n", "        return self._testing_iteration\n", "    \n", "    @property\n", "    def progress(self) -> float:\n", "        \"\"\"A number indicating the overall progress of the training\n", "        \n", "        Returns:\n", "            A floating point number between 0 and 1\n", "        \"\"\"        \n", "        n_iterations_per_epoch = np.ceil(self.size_training / self.batch_size) + \\\n", "                                 np.ceil(self.size_validation / self.batch_size)\n", "        n_iterations_total = self._n_epochs * n_iterations_per_epoch\n", "\n", "        iteration = self.epoch * n_iterations_per_epoch + \\\n", "                    self.training_iteration + self.validation_iteration\n", "        \n", "        progress = min(iteration/(n_iterations_total - 1), 1.0)\n", "        return progress\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["layer_classes = {\n", "    'DataRandom_Random_1': DataRandom_Random_1,\n", "    'DeepLearningFC_Dense_1': DeepLearningFC_Dense_1,\n", "    'DeepLearningFC_Dense_2': DeepLearningFC_Dense_2,\n", "    'ProcessReshape_Reshape_1': ProcessReshape_Reshape_1,\n", "    'DataData_MNIST': DataData_MNIST,\n", "    'ProcessReshape_Reshape_2': ProcessReshape_Reshape_2,\n", "    'MathSwitch_Switch_1': MathSwitch_Switch_1,\n", "    'DeepLearningFC_Dense_3': DeepLearningFC_Dense_3,\n", "    'DeepLearningFC_Is_Real': DeepLearningFC_Is_Real,\n", "    'TrainGan_GAN_1': TrainGan_GAN_1,\n", "}\n", "\n", "edges = {\n", "    ('DataRandom_Random_1', 'DeepLearningFC_Dense_1'),\n", "    ('DeepLearningFC_Dense_1', 'DeepLearningFC_Dense_2'),\n", "    ('DeepLearningFC_Dense_2', 'ProcessReshape_Reshape_1'),\n", "    ('ProcessReshape_Reshape_1', 'MathSwitch_Switch_1'),\n", "    ('DataData_MNIST', 'ProcessReshape_Reshape_2'),\n", "    ('ProcessReshape_Reshape_2', 'MathSwitch_Switch_1'),\n", "    ('MathSwitch_Switch_1', 'DeepLearningFC_Dense_3'),\n", "    ('DeepLearningFC_Dense_3', 'DeepLearningFC_Is_Real'),\n", "    ('DeepLearningFC_Is_Real', 'TrainGan_GAN_1'),\n", "}\n", "\n", "conn_info = {\n", "    'DataRandom_Random_1:DeepLearningFC_Dense_1': [('output', 'input')],\n", "    'DeepLearningFC_Dense_1:DeepLearningFC_Dense_2': [('output', 'input')],\n", "    'DeepLearningFC_Dense_2:ProcessReshape_Reshape_1': [('output', 'input')],\n", "    'ProcessReshape_Reshape_1:MathSwitch_Switch_1': [('output', 'input1')],\n", "    'DataData_MNIST:ProcessReshape_Reshape_2': [('output', 'input')],\n", "    'ProcessReshape_Reshape_2:MathSwitch_Switch_1': [('output', 'input2')],\n", "    'MathSwitch_Switch_1:DeepLearningFC_Dense_3': [('output', 'input')],\n", "    'DeepLearningFC_Dense_3:DeepLearningFC_Is_Real': [('output', 'input')],\n", "    'DeepLearningFC_Is_Real:TrainGan_GAN_1': [('output', 'input')],\n", "}\n", "\n", "\n", "iterator = graph.training_nodes[0].layer_instance.run(graph)\n", "result = None\n", "sentinel = object()\n", "while result is not sentinel:\n", "    result = next(iterator, sentinel)\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.2"}}, "nbformat": 4, "nbformat_minor": 4}